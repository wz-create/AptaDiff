#!/usr/bin/env python
# coding: utf-8
from functools import lru_cache
from collections import Counter, defaultdict
import re
import datetime
import torch
import numpy as np
import pickle
from pathlib import Path
from enum import IntEnum
from matplotlib import pyplot as plt
from itertools import product
import pandas as pd
# import matplotlib
# from numba import cuda
from tqdm.auto import tqdm
from multiprocessing import Pool

import logging
logger = logging.getLogger(__name__)


class State(IntEnum):
    M = 0
    I = 1
    D = 2


class Transition(IntEnum):
    M2M = 0
    M2I = 1
    M2D = 2
    I2M = 3
    I2I = 4
    D2M = 5
    D2D = 6


class nt_index(IntEnum):
    A = 0
    T = 1
    G = 2
    C = 3
    PAD = 4
    SOS = 5
    EOS = 6
    U = 1


def one_hot_index(seq):
    return [int(nt_index[char]) for char in seq]


def one_hot_encode(nucleotide, padding=0):
    """ 对于输入的字符串以独热的numpy数组形式返回
    """
    # 指定padding的大小
    arr = np.vstack((np.eye(4), np.ones(4)[None, :]*0.25))
    return arr[one_hot_index("N"*padding + nucleotide + "N"*padding)].T


class SNV(IntEnum):
    Mutation = 0
    Insertion = 1
    Deletion = 2


class SequenceGenerator():
    def __init__(self, num_motifs=1, motif_length=10, motifs=None,
                 target_length=20, fix_random_region_length=True, error_rate=0, generate_motifs=True, middle_insert_range=[2, 6],
                 seed=0, add_primer=True, forward_primer="AAAAA", reverse_primer="GGGGG", one_side_proba=0.5, paired=False):
        np.random.seed(seed)

        if generate_motifs:
            self.motifs = ["".join(np.random.choice(
                list("ATGC"), motif_length)) for _ in range(num_motifs)]
        else:
            self.motifs = motifs

        self.error_indices = 1 + \
            np.argsort(np.random.random(size=motif_length-1))[:3]
        self.mut_idx, self.ins_idx, self.del_idx = self.error_indices

        logger.info(f"error rate is {error_rate*100:.1f}%")
        for idx, motif in enumerate(self.motifs):
            seq = [ch for ch in motif]
            mut = self.mutate(seq[self.mut_idx])
            if error_rate != 0:
                seq[self.mut_idx] = f"[{seq[self.mut_idx]}>{mut}]"
                seq[self.ins_idx] = f"[+]{seq[self.ins_idx]}"
                seq[self.del_idx] = f"{seq[self.del_idx].lower()}"
            seq = "".join(seq)
            logger.info(f"motif {idx} is {seq}")

        self.num_motifs = num_motifs
        self.error_rate = error_rate
        self.target_length = target_length
        self.forward_primer = forward_primer
        self.reverse_primer = reverse_primer
        self.add_primer = add_primer

        self.one_side_proba = one_side_proba
        self.middle_insert_range = middle_insert_range
        self.paired = paired

    def mutate(self, char):
        return "TGCA"["ATGC".index(char)]

    def sample_motif(self, n):
        motif_indices = np.random.randint(self.num_motifs, size=n)
        has_errors = np.random.random(size=n) < self.error_rate
        # mutation, insertion, deletion
        error_types = np.random.choice(SNV, size=n)
        sequences = []
        for motif_index, has_error, error_type in zip(motif_indices, has_errors, error_types):
            motif = self.motifs[motif_index]
            seq = [ch for ch in motif]
            if has_error:
                if error_type == SNV.Mutation:
                    seq[self.mut_idx] = self.mutate(seq[self.mut_idx])
                elif error_type == SNV.Insertion:
                    seq[self.ins_idx] = np.random.choice(
                        list("ATGC")) + seq[self.ins_idx]
                elif error_type == SNV.Deletion:
                    seq[self.del_idx] = ""
                else:
                    raise NotImplementedError
            seq = "".join(seq)
            sequences.append(seq)
        return sequences, motif_indices.tolist()

    def sample(self, n=1, with_indices=True):
        motifs, motif_indices = self.sample_motif(n)
        sequences = []
        paired_indices = []
        for seq in motifs:
            if self.paired:
                seq, idx = self.insert_in_the_middle(
                    seq, nrange=self.middle_insert_range, one_side_proba=self.one_side_proba)
                paired_indices += [idx]
            random_region = "".join(np.random.choice(
                list("ATGC"), size=self.target_length-len(seq)))
            l = np.random.randint(len(random_region))
            if self.add_primer:
                sequences.append(
                    self.forward_primer + random_region[:l] + seq + random_region[l:] + self.reverse_primer)
            else:
                sequences.append(random_region[:l] + seq + random_region[l:])

        if self.paired and with_indices:
            return sequences, motif_indices, paired_indices
        elif with_indices:
            return sequences, motif_indices
        return sequences

    def insert_in_the_middle(self, sequence, nrange=[2, 6], one_side_proba=0.5):
        n = np.random.randint(*nrange)
        if np.random.random() < one_side_proba:
            if np.random.choice(["l", "r"]) == "l":
                l_motif = sequence[:len(sequence)//2]
                r_motif = ""
                idx = 1
            else:
                l_motif = ""
                r_motif = sequence[len(sequence)//2:]
                idx = 2
        else:
            l_motif = sequence[:len(sequence)//2]
            r_motif = sequence[len(sequence)//2:]
            idx = 0
        return l_motif + "".join(np.random.choice(list("ATGC"), size=n)) + r_motif, idx







def local_alignment(s1, s2, print_result=False, global_alignment=False):
    GAP_COST = -1
    MATCH_COST = +1
    MISMATCH_COST = -1

    # [[(0,0) for i in range(len(s2)+1)] for j in range(len(s1)+1)]
    dp = np.zeros((len(s1) + 1, len(s2) + 1, 2), dtype=np.int)

    def s(c1, c2):
        return MATCH_COST if c1 == c2 else MISMATCH_COST

    def idx_max(*args):
        argmax_i = np.argmax(args)
        return argmax_i, args[argmax_i]

    def concat(ret, a, b):
        return a + ret[0], b + ret[1]

    OBJ = 1
    POINTER_INDEX_TO_COND = ["MATCH", "S1_GAP", "S2_GAP", "END"]

    # DP
    for i1, c1 in enumerate(s1):
        for i2, c2 in enumerate(s2):
            l = [dp[i1][i2][OBJ] + s(c1, c2),
                 dp[i1][i2 + 1][OBJ] + GAP_COST,
                 dp[i1 + 1][i2][OBJ] + GAP_COST]
            if not global_alignment:
                l += [0]
            dp[i1 + 1][i2 + 1] = idx_max(*l)

    # local_alignment_traceback
    # logger.info(list(zip(*np.where(dp[:,:,OBJ]==dp[:,:,OBJ].max()))))
    result = []
    if global_alignment:
        traceback_starts = [(len(s1), len(s2))]
    else:
        traceback_starts = list(
            zip(*np.where(dp[:, :, OBJ] == dp[:, :, OBJ].max())))
    for i1, i2 in traceback_starts:
        traceback_pointer = dp[i1, i2, 0]
        ret = ["", ""]
        while True:
            if print_result:
                logger.info(i1, i2, POINTER_INDEX_TO_COND[traceback_pointer])
            if POINTER_INDEX_TO_COND[traceback_pointer] == "MATCH":
                i1 -= 1
                i2 -= 1
                if i1 < 0 or i2 < 0:
                    break
                if s1[i1] != s2[i2]:
                    ret = concat(ret, s1[i1].lower(), s2[i2].lower())
                else:
                    ret = concat(ret, s1[i1], s2[i2])
                traceback_pointer = dp[i1, i2, 0]
            elif POINTER_INDEX_TO_COND[traceback_pointer] == "S1_GAP":
                i1 -= 1
                if i1 < 0 or i2 < 0:
                    break
                ret = concat(ret, s1[i1], "-")
                traceback_pointer = dp[i1, i2, 0]
            elif POINTER_INDEX_TO_COND[traceback_pointer] == "S2_GAP":
                i2 -= 1
                if i1 < 0 or i2 < 0:
                    break
                ret = concat(ret, "-", s2[i2])
                traceback_pointer = dp[i1, i2, 0]
            else:
                break
        if print_result:
            logger.info(ret, dp[:, :, OBJ].max())
        result.append(ret)
    return dp[:, :, OBJ].max(), result


# from https://rosettacode.org/wiki/Levenshtein_distance#Memoized_recursive_version_2
@lru_cache(maxsize=2**26)
def edit_distance(s, t):
    if not s:
        return len(t)
    if not t:
        return len(s)
    if s[0] == t[0]:
        return edit_distance(s[1:], t[1:])
    l1 = edit_distance(s, t[1:])
    l2 = edit_distance(s[1:], t)
    l3 = edit_distance(s[1:], t[1:])
    return 1 + min(l1, l2, l3)


def get_complement_sequence(seq):
    return "".join(reversed([list("ATGC")["TACG".index(char)] for char in seq]))


class ProfileHMMSampler():
    def __init__(self, transition_proba, emission_proba, proba_is_log=False):
        self.e = emission_proba  # 发射概率，隐藏状态下处于观测状态的概率
        self.a = transition_proba  # 转移概率，下一时刻从状态qi->qi+1的概率
        if proba_is_log:
            self.e = np.exp(self.e)
            self.a = np.exp(self.a)
        self.e = self.e / np.sum(self.e, axis=1)[:, None]

    def sample(self, sequence_only=False, debug=False):
        idx, state = (0, State.M)
        states = [(idx, state)]
        seq = ""
        while True:
            if state == State.M:
                p = self.a[idx][np.array([
                    Transition.M2M.value,
                    Transition.M2I.value,
                    Transition.M2D.value])]
            elif state == State.I:
                p = np.stack([
                    self.a[idx][Transition.I2M.value],
                    self.a[idx][Transition.I2I.value],
                    0])
            elif state == State.D:
                p = np.stack([
                    self.a[idx][Transition.D2M.value],
                    0,
                    self.a[idx][Transition.D2D.value]])
            else:
                logger.info("something wrong")

            state = np.random.choice([State.M, State.I, State.D], p=p/sum(p))
            if state != State.I:
                idx += 1
            states.append((idx, state))
            if idx == self.a.shape[0]:
                break

            if state == State.M:
                # logger.info("{:.2f}, {:.2f}, {:.2f}, {:.2f}".format(*self.e[idx-1]))

                seq += np.random.choice(list("ATGC"), p=self.e[idx-1])
                if debug:
                    logger.info(idx, state, self.e[idx-1], seq[-1])
            elif state == State.I:
                seq += np.random.choice(list("atgc"))
            else:
                seq += "_"
        if not sequence_only:
            return states, seq
        else:
            return seq

    def most_probable(self, sequence_only=False):
        idx, state = (0, State.M)
        states = [(idx, state)]
        seq = ""
        while True:
            if state == State.M:
                p = self.a[idx][np.array([
                    Transition.M2M.value,
                    Transition.M2I.value,
                    Transition.M2D.value])]
            elif state == State.I:
                p = [
                    self.a[idx][Transition.I2M.value],
                    0,
                    0]
            elif state == State.D:
                p = [
                    self.a[idx][Transition.D2M.value],
                    0,
                    self.a[idx][Transition.D2D.value]]
            else:
                logger.info("something wrong")
            p[np.argmax(p)] += 1000000
            state = np.random.choice([State.M, State.I, State.D], p=p/sum(p))
            if state != State.I:
                idx += 1
            states.append((idx, state))

            if idx == self.a.shape[0]:
                break

            if state == State.M:
                # logger.info("{:.2f}, {:.2f}, {:.2f}, {:.2f}".format(*self.e[idx-1]))
                p = np.copy(self.e[idx-1])
                p[np.argmax(p)] += 100000
                seq += np.random.choice(list("ATGC"), p=p/sum(p))
            elif state == State.I:
                seq += "N"
            else:
                seq += "_"
        if not sequence_only:
            return states, seq
        else:
            return seq

    def calc_seq_proba(self, seq: str):
        one_hot_seq = torch.tensor(one_hot_index(seq))
        model_len = self.e.shape[0]
        random_len = len(seq)

        e = np.log(self.e)
        a = np.log(self.a)

        F = torch.ones((3, model_len + 2, random_len + 1)) * (-100)

        # init
        F[0, 0, 0] = 0

        for i in range(random_len + 1):
            for j in range(model_len + 1):
                # State M
                if j*i != 0:
                    F[State.M, j, i] = e[j - 1][one_hot_seq[i - 1]] + \
                        torch.logsumexp(torch.stack((
                            a[j - 1, Transition.M2M] +
                            F[State.M, j - 1, i - 1],
                            a[j - 1, Transition.I2M] +
                            F[State.I, j - 1, i - 1],
                            a[j - 1, Transition.D2M] + F[State.D, j - 1, i - 1])), dim=0)

                # State I
                if i != 0:
                    F[State.I, j, i] = - 1.3863 + \
                        torch.logsumexp(torch.stack((
                            a[j, Transition.M2I] + F[State.M, j, i-1],
                            a[j, Transition.I2I] + F[State.I, j, i-1]
                        )), dim=0)

                # State D
                if j != 0:
                    F[State.D, j, i] = \
                        torch.logsumexp(torch.stack((
                            a[j - 1, Transition.M2D] + F[State.M, j - 1, i],
                            a[j - 1, Transition.D2D] + F[State.D, j - 1, i]
                        )), dim=0)

        F[State.M, model_len+1, random_len] = \
            torch.logsumexp(torch.stack((
                a[model_len, Transition.M2M] +
                F[State.M, model_len, random_len],
                a[model_len, Transition.I2M] +
                F[State.I, model_len, random_len],
                a[model_len, Transition.D2M] +
                F[State.D, model_len, random_len]
            )), dim=0)

        return F[State.M, model_len+1, random_len]


# class Result():
#     """実験結果の保存のためのクラス"""
#     # 用于保存实验结果
#     from visualization import provide_ax
#
#     def __init__(self,
#                  model,
#                  path_to_selex: str = None,
#                  experiment: SingleRound = None,
#                  path_to_result_csv: str = None,
#                  path_to_model: str = None,
#                  lazy_mu_eval=False,
#                  path_to_save_results=None,
#                  evaluated_X=None,
#                  evaluated_y=None,
#                  load_if_exists=False,
#                  min_count=1
#                  ):
#
#         if experiment is None:
#             self.experiment = SingleRound(path=Path(path_to_selex))
#         else:
#             self.experiment = experiment
#
#         if path_to_result_csv:
#             self.result_df = pd.read_csv(Path(path_to_result_csv))
#         else:
#             logger.info("skip loading training result")
#
#         if path_to_model:
#             model.load_state_dict(torch.load(path_to_model))
#         else:
#             logger.info("skip loading model parameters")
#
#         self.model = model
#
#         if path_to_save_results is None:
#             self.path_to_save_results = Path(
#                 "result_" + self.get_result_hash())
#         else:
#             self.path_to_save_results = Path(path_to_save_results)
#         if load_if_exists:
#             self.path_to_save_results.mkdir(parents=True, exist_ok=True)
#         else:
#             self.path_to_save_results.mkdir(parents=True)
#
#         self.is_phmm = "PHMM" in str(model.__class__)
#         self.min_count = min_count
#         if not lazy_mu_eval:
#             logger.info("evaluating mu")
#             self.get_mean_vectors_from_experiment()
#
#         self.evaluated_X = evaluated_X
#         self.evaluated_y = evaluated_y
#
#     def get_mean_vectors_from_experiment(self, get_raw_seq=False, force=False):
#         if not hasattr(self, "mus") or not hasattr(self, "seqs") or force:
#             loaders = self.experiment.get_dataloader(
#                 shuffle=False, min_count=self.min_count)
#             with torch.no_grad():
#                 self.model.eval()
#                 mus = []
#                 seqs = []
#                 for loader in loaders:
#                     for data in loader:
#                         _, mu, logvar = self.model(data, deterministic=True)
#                         mus += [*mu.detach().numpy()]
#                         for datum in data:
#                             seq = "".join(np.array(list("ATGC"))
#                                           [datum.numpy()])
#                             seqs.append(seq)
#
#             self.mus = np.stack(mus)
#             self.seqs = seqs
#
#         if get_raw_seq:
#             return self.mus, self.seqs
#         return self.mus
#
#     def get_result_hash(self):
#         # modelのパラメタが一意に決まっていてhashしやすいのでこれを利用する
#         import hashlib
#         b = "".join(["{:.2f}".format(i.flatten()[0].cpu().detach().numpy())
#                      for i in self.model.parameters()])
#         self.hash = hashlib.sha1(b.encode()).hexdigest()[:10]
#         logger.info(f"hash : {self.hash}")
#         return self.hash
#
#     def calc_gmm(self, dim=10, calc_times=100, force=False):
#         from sklearn.mixture import GaussianMixture
#         logger.info("calculating gmm centers")
#         X = self.mus
#         gmm_path = self.path_to_save_results/"gmm.pkl"
#         if gmm_path.exists():
#             logger.info(f"loading {gmm_path}")
#             with gmm_path.open("rb") as f:
#                 best_gmm = pickle.load(f)
#                 best_aic = best_gmm.aic(X)
#         else:
#             best_aic = np.inf
#             pbar = tqdm(range(calc_times))
#             for i in pbar:
#                 gmm = GaussianMixture(dim, covariance_type="full").fit(X)
#                 if gmm.aic(X) < best_aic:
#                     best_aic = gmm.aic(X)
#                     best_gmm = gmm
#                 pbar.set_description(
#                     "[" + "⠸⠴⠦⠇⠋⠙"[i % 6] + "]" + f"{best_aic:.2f}")
#
#             with gmm_path.open("wb") as f:
#                 pickle.dump(best_gmm, f)
#
#         logger.info(f"best aic : {best_aic}")
#         self.gmm = best_gmm
#         self.aic = best_aic
#         self.gmm_classes = best_gmm.predict(X)
#         self.gmm_centers = best_gmm.means_
#
#     def embed_sequences(self, sequences):
#         """
#         docstring
#         """
#         if type(sequences) == str:
#             sequences = [sequences]
#
#         # https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda/180
#         model_device = next(self.model.parameters()).device
#         with torch.no_grad():
#             self.model.eval()
#             mus = []
#             for sequence in sequences:
#                 recon, mu, logvar = self.model(
#                     torch.Tensor(
#                         [one_hot_index(sequence)],
#                         device=model_device).long())
#                 mus += [mu]
#         return torch.cat(mus)
#
#     @provide_ax
#     def plot_gmm(self, ax, fig=None, save=True, no_colors=False, no_gmm_centers=False):
#         if not hasattr(self, "gmm"):
#             logger.info("calculating gmm")
#             self.calc_gmm()
#         if len(self.gmm_classes) > 30000:
#             if no_colors:
#                 ax.scatter(*self.mus[:30000].T, s=2, c="silver")
#             else:
#                 ax.scatter(*self.mus[:30000].T, c=self.gmm_classes[:30000], s=2)
#         else:
#             if no_colors:
#                 ax.scatter(*self.mus.T, s=2, c="silver")
#             else:
#                 ax.scatter(*self.mus.T, c=self.gmm_classes, s=2)
#         if not no_gmm_centers:
#             ax.scatter(*self.gmm_centers.T, c="r", marker="*",
#                        s=10, zorder=50, label="gmm center")
#             for i, (x, y) in enumerate(self.gmm_centers):
#                 ax.text(x, y, f" {i}", ha="left", va="center", c="r", bbox=dict(
#                     facecolor='white', alpha=0.5), zorder=40)
#
#         ax.axis("square")
#         if save:
#             fig.savefig(self.path_to_save_results/'gmm.png')
#         return ax
#
#     @provide_ax
#     def plot_means(self, ax, fig=None, with_count=False, save=True, meshgrid=True):
#         if self.mus is None:
#             logger.info("running estimation")
#             self.get_mean_vectors_from_experiment()
#
#         if with_count:
#             c = Counter(
#                 self.experiment.get_filter_passed_sequences(random_only=True))
#             counts = [np.log2(c[seq]) for seq in self.seqs]
#             XY = self.mus[np.argsort(counts)]
#             if meshgrid:
#                 XY_ = []
#                 for xy, cnt in zip(XY, [c[seq] for seq in self.seqs]):
#                     XY_ += [xy]*int(np.log2(cnt))
#                 heatmap, xedges, yedges = np.histogram2d(
#                     * np.stack(XY_).T, bins=100, range=((-3, 3), (-3, 3)))
#             else:
#                 cs = ax.scatter(*XY.T, s=2, c=sorted(counts), vmin=0)
#                 cbar = fig.colorbar(cs)
#                 cbar.ax.set_title("log2 counts")
#                 legend = ax.legend(*cs.legend_elements(num=6),
#                                    loc="lower right", title="log2 count")
#                 ax.add_artist(legend)
#
#         else:
#             ax.scatter(*self.mus.T, s=2)
#         ax.axis("square")
#
#         if save:
#             fig.savefig(self.path_to_save_results/'means.png')
#         return ax
#
#     @provide_ax
#     def plot_bo(self, ax, fig=None, n_grid=101, save=True, with_index=True, plot_range=(-2, 2)):
#         from mpl_toolkits.axes_grid1 import make_axes_locatable
#         if not hasattr(self, "next_locations"):
#             self.get_bo_result()
#
#         self.bo_grid_x, self.bo_grid_y = np.meshgrid(
#             np.linspace(*plot_range, n_grid),
#             np.linspace(*plot_range, n_grid))
#         self.bo_X = np.stack(map(np.ravel, (self.bo_grid_x, self.bo_grid_y))).T
#
#         mu, sigma = self.bo.model.predict(self.bo_X)
#         self.bo_mu = mu
#         self.bo_sigma = sigma
#
#         cont = ax.contour(self.bo_grid_x, self.bo_grid_y, -
#                           (self.bo_mu-self.bo_sigma).reshape(n_grid, n_grid))
#         cont.clabel(fmt='%1.1f', fontsize=8)
#         divider = make_axes_locatable(ax)
#         cax = divider.append_axes('right', size='5%', pad=0.05)
#
#         ax.scatter(*self.evaluated_X.T,
#                    c=self.evaluated_y[:, 0], cmap="bwr_r", ec="grey", lw=0.5, zorder=40, label="evaluated")
#         if with_index:
#             for i, (x, y) in enumerate(self.evaluated_X):
#                 ax.text(x, y, " "+str(i), color="blue", va="center")
#
#         ax.scatter(*self.next_locations.T, marker="*",
#                    color="k", zorder=50, label="bo proposed")
#         if with_index:
#             for i, (x, y) in enumerate(self.next_locations):
#                 ax.text(x, y, f" {i}", color="k", zorder=51, va="center")
#         fig.colorbar(cont, cax=cax)
#
#         return ax
#
#     def get_bo_result(self, n=10, domain=(-2, 2), force_rerun=False):
#         if hasattr(self, "next_locations") and not force_rerun:
#             return self.next_locations
#         import GPyOpt
#         bo_path = self.path_to_save_results/"bo.pkl"
#         if bo_path.exists() and not force_rerun:
#             logger.info(f"loading {bo_path}")
#             with bo_path.open("rb") as f:
#                 self.bo = pickle.load(f)
#         else:
#             logger.info("calculating bo")
#             assert self.evaluated_X is not None and self.evaluated_y is not None,\
#                 "(N, d) array: `evaluated_X` and (N, 1) array: `evaluated_y` should be set"
#             self.domain = domain
#             self.constraints = [{"name": f"var_{i+1}",
#                                  "type": "continuous",
#                                  "domain": self.domain}
#                                 for i in range(self.evaluated_X.shape[1])]
#
#             self.bo = GPyOpt.methods.BayesianOptimization(
#                 None,
#                 domain=self.constraints,
#                 model_type='GP',
#                 acquisition_type='LCB',
#                 evaluator_type="local_penalization",
#                 batch_size=n,
#                 X=self.evaluated_X,
#                 Y=self.evaluated_y)
#             with bo_path.open("wb") as f:
#                 pickle.dump(self.bo, f)
#
#         self.next_locations = self.bo.suggest_next_locations()
#         return self.next_locations
#
#     def _points_to_score(self, points, eval_max=256):
#         a, e_m = self.model.decoder(points)
#         a = a.detach().numpy()
#         e_m = e_m.detach().numpy()
#
#         logger.info(
#             f"calculating most probable sequences up to {eval_max} candidates")
#         pbar = tqdm(range(len(points)))
#         scores = []
#         for j in pbar:
#             sampler = ProfileHMMSampler(a[j], e_m[j], proba_is_log=True)
#             seq_pattern = sampler.most_probable()[1].replace(
#                 "_", "").replace("N", "*")
#             products = product(*[list("ATGC")
#                                  for _ in range(seq_pattern.count("*"))])
#
#             rets = []
#             for nt_set in products:
#                 ret = ""
#                 for part, nt in zip(seq_pattern.split("*"), list(nt_set)+[""]):
#                     ret += part+nt
#                 rets += [ret]
#             if len(rets) > eval_max:
#                 rets = [rets[idx] for idx in np.argsort(
#                     np.random.randn(len(rets)))[:eval_max]]
#             with Pool() as p:
#                 probas = p.map(sampler.calc_seq_proba, rets)
#
#             most_probable_seq, min_value = sorted(
#                 list(zip(rets, probas)), key=lambda x: x[1])[0]
#             min_value = min_value.item()
#             scores += [(seq_pattern, most_probable_seq, min_value)]
#         self.scores = scores
#         return scores
#
#     def _save_scores(self, scores, model_type, filename, id_header="", force=False, loc=None):
#         from datetime import datetime
#         if filename is not None:
#             assert not ((not force) and (self.path_to_save_results /
#                                          filename).exists()), "file exists. to override, try: 'force=True'"
#             with open(self.path_to_save_results / filename, "w") as f:
#                 f.write("id,method,max_model,max_seq,log_proba")
#                 if loc is not None:
#                     f.write(",pos_x,pos_y\n")
#                 else:
#                     f.write("\n")
#
#                 for i, (seq_pattern, most_probable_seq, min_value) in enumerate(scores):
#                     id_str = datetime.now().strftime(
#                         "%-y%m%d_") + id_header + f"_{i}"
#                     f.write(
#                         f"{id_str},{model_type},{seq_pattern},{most_probable_seq},{min_value}")
#                     if loc is not None:
#                         f.write(f",{loc[i][0]},{loc[i][1]}\n")
#                     else:
#                         f.write("\n")
#             logger.info(f"saved to {self.path_to_save_results}/{filename}")
#
#     def get_gmm_probable_sequences(self, filename=None):
#         if hasattr(self, "scores"):
#             return [most_probable for seq_pattern, most_probable, min_value in self.scores]
#         if not hasattr(self, "gmm_centers"):
#             self.plot_gmm()
#             # self.calc_gmm()
#
#         scores = self._points_to_score(torch.Tensor(self.gmm_centers))
#
#         self._save_scores(scores, "GMM", filename)
#
#         return [most_probable for seq_pattern, most_probable, min_value in scores]
#
#     def plot_training_result(self, nwarmup=100, save=True, fig=None, axes=None):
#         from visualization import get_ax
#         if axes is not None and fig is not None:
#             ax, ay = axes
#         fig, (ax, ay) = get_ax(row_col=(2, 1), return_fig=True)
#
#         # for mean plot
#         length = 20
#         v = np.sin(np.arange(length)/(length-1)*np.pi)
#         v /= sum(v)
#
#         i = 0
#         for arr_name in ["test_loss", "test_recon", "train_loss"]:
#             cmap = plt.get_cmap("Paired")
#             test_losses = np.array(self.result_df[arr_name])
#             epochs = np.arange(len(test_losses))
#             conv_indices, conv_values = np.array(
#                 epochs[:-length+1])+length//2, np.convolve(test_losses, v, mode='valid')
#             ax.plot(test_losses, c=cmap(i))
#
#             ax.plot(conv_indices, conv_values, c=cmap(i+1), label=arr_name)
#             ax.plot(conv_indices[-1], conv_values[-1], marker='.', c=cmap(i+1))
#             ax.text(np.argmin(test_losses) + 1, np.min(test_losses),
#                     f"←{np.min(test_losses):.2f}", ha="left", va="top", c=cmap(i+1), rotation=-45, fontsize=8,
#                     bbox=dict(ec=(1, 1, 1, 1), facecolor="w", alpha=1, pad=0))
#             i += 2
#
#         for arr_name in ["test_kld"]:
#             cmap = plt.get_cmap("Paired")
#             test_losses = np.array(self.result_df[arr_name])
#             epochs = np.arange(len(test_losses))
#             conv_indices, conv_values = np.array(
#                 epochs[:-length+1])+length//2, np.convolve(test_losses, v, mode='valid')
#             ay.plot(test_losses, c=cmap(i))
#
#             ay.plot(conv_indices, conv_values, c=cmap(i+1), label=arr_name)
#             ay.plot(conv_indices[-1], conv_values[-1], marker='.', c=cmap(i+1))
#
#             i += 2
#
#         min_test_loss = min(self.result_df.test_loss)
#         min_test_recon = min(self.result_df.test_recon)
#         dloss = abs(min_test_loss-min_test_recon)
#         ax.set_ylim(min_test_recon-dloss*0.6, min_test_loss+dloss*0.6)
#         ay.set_ylim(-dloss*0.1, dloss*2.1)
#
#         ax.plot((nwarmup, nwarmup), (0, 100), "--",
#                 c="gray", label="profile hmm warmup")
#         ay.plot((nwarmup, nwarmup), (0, 100), "--",
#                 c="gray", label="profile hmm warmup")
#         ay.set_xlabel("epochs")
#         ax.set_ylabel("loss")
#         ay.set_ylabel("loss")
#
#         ax.legend()
#         ay.legend()
#         ax.set_title("training result")
#         if save:
#             fig.savefig(self.path_to_save_results/'training.png')
#
#
# class Experiments():
#     """whole selex experiment"""
#
#     def __init__(self, read_paths: list, has_same_adapters=True, k=3):
#         adapters = None
#         self.rounds = []
#         for read_path in sorted(read_paths):
#             logger.info(f"reading ... {read_path}")
#             path = Path(read_path)
#             if adapters is not None and has_same_adapters:
#                 fwd, rev = adapters
#                 single_round = SingleRound(path=path, name=path.stem,
#                                            forward_adapter=fwd, reverse_adapter=rev)
#             else:
#                 single_round = SingleRound(path=path, name=path.stem)
#                 adapters = single_round.get_adapters()
#             self.rounds += [single_round]
#         self.k = k
#         self.kmer_list = sorted(list("".join(l)
#                                      for l in product(list("ATGC"), repeat=k)))
#
#     def has_exact_match(self, sequences):
#         if type(sequences) is str:
#             sequences = [sequences]
#         results = dict()
#         for sequence in sequences:
#             appeared = set()
#             for experiment in self.rounds:
#                 if sequence in set(experiment.get_filter_passed_sequences(random_only=True)):
#                     appeared |= {experiment.name}
#             results[sequence] = appeared
#         return results
#
#     def kmer(self, seq):
#         return [seq[i: i+self.k] for i in range(len(seq)-self.k+1)]
#
#     def kmer_count(self, seq, to_list=False):
#         from collections import Counter
#         c = Counter(self.kmer(seq))
#         if to_list:
#             return [c[kmer] for kmer in self.kmer_list]
#         return c
#
#     def save_frequencies(self, save_path, sequences, min_count=2, idx_header=""):
#         from functools import partial
#
#         if type(sequences) is str:
#             sequences = [sequences]
#         pbar = tqdm(total=len(sequences))
#
#         whole_read_counts = Counter(
#             [seq for experiment in self.rounds for seq in experiment.get_filter_passed_sequences(random_only=True)])
#         whole_reads = [x[0] for x in filter(
#             lambda x:x[1] >= min_count, whole_read_counts.most_common())]
#         pbar.set_description("calculating kmer distribution")
#         if not hasattr(self, "whole_reads_kmer"):
#             self.whole_reads_kmer = np.stack(
#                 [np.array(self.kmer_count(seq, to_list=True)) for seq in whole_reads])
#
#         with open(save_path, "w") as f:
#             f.write(
#                 "idx\tgenerate_seq\texact_match_in\tnearest_selex_seq\tscore\tselex_aligned\tgenerate_aligned\t")
#             f.write(
#                 "\t".join([experiment.name for experiment in self.rounds])+"\n")
#
#             # for sequence in query
#             for idx, most_probable_seq in enumerate(sequences):
#                 pbar.set_description("taking alignments")
#                 func = partial(local_alignment, s2=most_probable_seq)
#                 with Pool() as p:
#                     alignments = p.map(func, whole_reads)
#                 arr = np.array(list(zip(*alignments))[0])
#
#                 matches = self.has_exact_match(most_probable_seq)[
#                     most_probable_seq]
#
#                 pbar.set_description("writing to file")
#                 for jdx in np.argwhere(arr == max(arr)).flatten():
#                     score = alignments[jdx][0]
#                     opt_seq_align, check_seq_align = alignments[jdx][1][0]
#                     f.write(
#                         f"{idx_header}{idx}\t{most_probable_seq}\t{matches}\t{whole_reads[jdx]}\t{score}\t{opt_seq_align}\t{check_seq_align}")
#
#                     for experiment in self.rounds:
#                         c = experiment.get_filter_passed_sequences_and_count(
#                             random_only=True)
#                         if whole_reads[jdx] in c.keys():
#                             ratio = f"\t{c[whole_reads[jdx]] / len(experiment.get_filter_passed_sequences()):.6f}"
#                             f.write(ratio)
#                         else:
#                             f.write("\t0.000000")
#                     f.write("\n")
#                 pbar.update(1)
#         return pd.read_table(save_path)

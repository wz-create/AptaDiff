#!/usr/bin/env python
# coding: utf-8
import copy
import os.path
from functools import lru_cache
from collections import Counter, defaultdict
import re
import datetime
import torch
import numpy as np
import pickle
from pathlib import Path
from enum import IntEnum
from matplotlib import pyplot as plt
from itertools import product
from typing import List
import pandas as pd
import matplotlib
from tqdm.auto import tqdm
from multiprocessing import Pool
from typing import Optional, Dict, List, Union

from vae.raptgen.visualization import draw_most_probable

import logging

logger = logging.getLogger(__name__)


class State(IntEnum):
    M = 0
    I = 1
    D = 2


class Transition(IntEnum):
    M2M = 0
    M2I = 1
    M2D = 2
    I2M = 3
    I2I = 4
    D2M = 5
    D2D = 6


class nt_index(IntEnum):
    A = 0
    T = 1
    G = 2
    C = 3
    PAD = 4
    SOS = 5
    EOS = 6
    U = 1
    # N = 7 # N数据处理或者不处理，都可以做实验进行验证。


def one_hot_index(seq):
    return [int(nt_index[char]) for char in seq]


def one_hot_encode(nucleotide, padding=0):
    """入力された文字列に対してOne-hotなnp形式を返す
    """
    # パディングの大きさを指定する
    arr = np.vstack((np.eye(4), np.ones(4)[None, :] * 0.25))
    return arr[one_hot_index("N" * padding + nucleotide + "N" * padding)].T


class SNV(IntEnum):
    Mutation = 0
    Insertion = 1
    Deletion = 2


class SequenceGenerator():
    def __init__(self, num_motifs=1, motif_length=10, motifs=None,
                 target_length=20, fix_random_region_length=True, error_rate=0, generate_motifs=True,
                 middle_insert_range=[2, 6],
                 seed=0, add_primer=True, forward_primer="AAAAA", reverse_primer="GGGGG", one_side_proba=0.5,
                 paired=False):
        np.random.seed(seed)

        if generate_motifs:
            self.motifs = ["".join(np.random.choice(
                list("ATGC"), motif_length)) for _ in range(num_motifs)]
        else:
            self.motifs = motifs

        self.error_indices = 1 + \
                             np.argsort(np.random.random(size=motif_length - 1))[:3]
        self.mut_idx, self.ins_idx, self.del_idx = self.error_indices

        logger.info(f"error rate is {error_rate * 100:.1f}%")
        for idx, motif in enumerate(self.motifs):
            seq = [ch for ch in motif]
            mut = self.mutate(seq[self.mut_idx])
            if error_rate != 0:
                seq[self.mut_idx] = f"[{seq[self.mut_idx]}>{mut}]"
                seq[self.ins_idx] = f"[+]{seq[self.ins_idx]}"
                seq[self.del_idx] = f"{seq[self.del_idx].lower()}"
            seq = "".join(seq)
            logger.info(f"motif {idx} is {seq}")

        self.num_motifs = num_motifs
        self.error_rate = error_rate
        self.target_length = target_length
        self.forward_primer = forward_primer
        self.reverse_primer = reverse_primer
        self.add_primer = add_primer

        self.one_side_proba = one_side_proba
        self.middle_insert_range = middle_insert_range
        self.paired = paired

    def mutate(self, char):
        return "TGCA"["ATGC".index(char)]

    def sample_motif(self, n):
        motif_indices = np.random.randint(self.num_motifs, size=n)
        has_errors = np.random.random(size=n) < self.error_rate
        # mutation, insertion, deletion
        error_types = np.random.choice(SNV, size=n)
        sequences = []
        for motif_index, has_error, error_type in zip(motif_indices, has_errors, error_types):
            motif = self.motifs[motif_index]
            seq = [ch for ch in motif]
            if has_error:
                if error_type == SNV.Mutation:
                    seq[self.mut_idx] = self.mutate(seq[self.mut_idx])
                elif error_type == SNV.Insertion:
                    seq[self.ins_idx] = np.random.choice(
                        list("ATGC")) + seq[self.ins_idx]
                elif error_type == SNV.Deletion:
                    seq[self.del_idx] = ""
                else:
                    raise NotImplementedError
            seq = "".join(seq)
            sequences.append(seq)
        return sequences, motif_indices.tolist()

    def sample(self, n=1, with_indices=True):
        motifs, motif_indices = self.sample_motif(n)
        sequences = []
        paired_indices = []
        for seq in motifs:
            if self.paired:
                seq, idx = self.insert_in_the_middle(
                    seq, nrange=self.middle_insert_range, one_side_proba=self.one_side_proba)
                paired_indices += [idx]
            random_region = "".join(np.random.choice(
                list("ATGC"), size=self.target_length - len(seq)))
            l = np.random.randint(len(random_region))
            if self.add_primer:
                sequences.append(
                    self.forward_primer + random_region[:l] + seq + random_region[l:] + self.reverse_primer)
            else:
                sequences.append(random_region[:l] + seq + random_region[l:])

        if self.paired and with_indices:
            return sequences, motif_indices, paired_indices
        elif with_indices:
            return sequences, motif_indices
        return sequences

    def insert_in_the_middle(self, sequence, nrange=[2, 6], one_side_proba=0.5):
        n = np.random.randint(*nrange)
        if np.random.random() < one_side_proba:
            if np.random.choice(["l", "r"]) == "l":
                l_motif = sequence[:len(sequence) // 2]
                r_motif = ""
                idx = 1
            else:
                l_motif = ""
                r_motif = sequence[len(sequence) // 2:]
                idx = 2
        else:
            l_motif = sequence[:len(sequence) // 2]
            r_motif = sequence[len(sequence) // 2:]
            idx = 0
        return l_motif + "".join(np.random.choice(list("ATGC"), size=n)) + r_motif, idx


def get_reads_with_id_prefix(path, prefix_on, prefix_off):
    reads = []
    read = ""
    switch = False
    with path.open() as f:
        for line in f.readlines():
            if line[0] == prefix_off:
                switch = False
                reads.append(read)
                read = ""
            if switch:
                read = read + line.strip()
            if line[0] == prefix_on:
                switch = True
                read = ""
        # write last read line
        reads.append(read)
    return reads


def read_fasta(path):
    return get_reads_with_id_prefix(Path(path), ">", ">")


def read_fastq(path):
    return get_reads_with_id_prefix(Path(path), "@", "+")


def process_csv(data: pd.DataFrame, columns: Union[List, str]) -> List:
    output = data[columns].values.tolist()
    return output


def read_csv(path, read_tag='', label_tag=''):
    data = pd.read_csv(path, encoding='utf8')
    reads = []
    labels = []
    output = []
    if label_tag and label_tag not in data.columns:
        raise Exception(f'Label tag {label_tag} not in input data!')
    if label_tag:
        tmp = process_csv(data, [read_tag, label_tag])
        for i, j in tmp:
            if 'N' not in i:
                reads.append(i)
                labels.append(j)
                output.append([i, j])
        return reads, labels, output
    else:
        output = process_csv(data, read_tag)
        return [i for i in output if 'N' not in i], labels, []


class SingleRound:
    """pass path or raw_reads to make class of selex experiment per round.
    """

    def __init__(self, raw_reads: list = None, forward_adapter=None, reverse_adapter=None, name=None, tolerance=0,
                 path: str = None, max_len=None, label_tag='', seq_tag='seq'):
        assert path is not None or raw_reads is not None, "either path or raw_reads has to be specified"
        if path:
            path = Path(path)
            if path.suffix == ".fastq":
                logger.info("reading fastq format sequence")
                raw_reads = read_fastq(path)
            elif path.suffix in {".fasta", ".fa"}:
                logger.info("reading fasta format sequence")
                raw_reads = read_fasta(path)
            elif path.suffix in {".csv"}:
                logger.info("reading csv format sequence")
                raw_reads, raw_labels, raw_all = read_csv(path, seq_tag, label_tag)
                if label_tag:
                    self.raw_labels = raw_labels
                    self.raw_all = raw_all
            else:
                logger.critical(
                    "please specify a file with csv, fasta or fastq format")
                quit()

        self.raw_reads = raw_reads
        self.calc_target_length()
        self.max_len = max_len

        if forward_adapter is None or reverse_adapter is None:
            logger.info("adapter info not provided. estimating value")
            self.calc_experimental_settings()
        else:
            logger.info(
                f"sequence design : {forward_adapter}-[random]-{reverse_adapter}")
            self.set_adapters(forward_adapter, reverse_adapter,
                              self.max_len is not None)

        if name:
            self.name = name
        else:
            self.name = re.sub(r'[-\.\:]', "",
                               str(datetime.datetime.now())).replace(" ", "_")
        logger.info(f"experiment name : {self.name}")
        self.tolerance = tolerance

    def get_adapters(self):
        return self.forward_adapter, self.reverse_adapter

    def set_adapters(self, forward_adapter: str, reverse_adapter: str, set_max_len=False):
        self.forward_adapter = forward_adapter
        self.forward_adapter_length = len(forward_adapter)

        self.reverse_adapter = reverse_adapter
        self.reverse_adapter_length = len(reverse_adapter)

        self.random_region_length = self.target_length - \
                                    self.reverse_adapter_length - self.forward_adapter_length
        if set_max_len:
            self.random_region_length = self.max_len

    def calc_target_length(self):
        from collections import Counter, defaultdict
        self.read_counter = Counter(self.raw_reads)

        # calc most common length
        d = defaultdict(int)
        for key, value in self.read_counter.items():
            d[len(key)] += value
        self.target_length = sorted(d.items(), key=lambda x: -x[1])[0][0]

    def calc_experimental_settings(self):
        """calculate sequence adapters in a heuristic way
        """

        # fwd
        max_count = None
        est_adapter = ""
        for i in range(1, self.target_length):
            d = defaultdict(int)
            for seq, count in self.read_counter.most_common():
                if len(seq) < i or len(d) > 100 and seq[:i] not in d.keys():
                    continue
                d[seq[:i]] += count
            top_seq, top_count = sorted(d.items(), key=lambda x: -x[1])[0]
            if max_count is not None and top_count < max_count * 0.5:  # heuristics
                logger.info(
                    f"estimated forward adapter len is {i - 1} : {est_adapter}")
                break
            max_count = sorted(d.items(), key=lambda x: -x[1])[0][1]
            if max_count < sum(self.read_counter.values()) * 0.5:
                logger.info(
                    f"no match found.")
                break
            est_adapter = top_seq
        fwd_len = i - 1
        fwd_adapter = est_adapter

        # rev
        max_count = None
        est_adapter = ""
        for i in range(1, self.target_length):
            d = defaultdict(int)
            for seq, count in self.read_counter.most_common():
                if len(seq) < i or len(d) > 100 and seq[-i:] not in d.keys():
                    continue
                d[seq[-i:]] += count
            top_seq, top_count = sorted(d.items(), key=lambda x: -x[1])[0]
            if max_count is not None and top_count < max_count * 0.5:  # heuristics
                logger.info(
                    f"estimated reverse adapter len is {i - 1} : {est_adapter}")
                break
            max_count = sorted(d.items(), key=lambda x: -x[1])[0][1]
            if max_count < sum(self.read_counter.values()) * 0.5:
                logger.info(
                    f"no match found.")
                break
            est_adapter = top_seq
        rev_len = i - 1
        rev_adapter = est_adapter

        rand_len = self.target_length - rev_len - fwd_len

        logger.info(
            f"filtering with : {fwd_adapter}({fwd_len}N)-{rand_len}N-{rev_adapter}({rev_len}N)")

        # write estimated experimental settings
        self.set_adapters(fwd_adapter, rev_adapter, self.max_len is not None)

    def get_sequences_and_count(self):
        c = Counter(self.raw_reads)
        return c.most_common()

    def get_filter_passed_sequences_and_count(self, random_only=False):
        if random_only:
            return {self.cut_adapters(key): value for key, value in self.get_sequences_and_count()}
        else:
            c = Counter(self.get_filter_passed_sequences())
            return c.most_common()

    def filter_reads_function(self, read):
        has_forward = read[: self.forward_adapter_length] == self.forward_adapter \
                      or self.forward_adapter_length == 0
        has_reverse = read[-self.reverse_adapter_length:] == self.reverse_adapter \
                      or self.reverse_adapter_length == 0
        match_random_region_len = abs(
            len(read) - self.target_length) <= self.tolerance
        return has_forward and has_reverse and match_random_region_len

    def filter_all_function(self, all_info):
        read, label = all_info
        return self.filter_reads_function(read)

    def get_filter_passed_sequences(self, random_only=False, has_label=False):
        if has_label:
            filter_passed = list(filter(self.filter_all_function, self.raw_all))
            self.filter_passed = [i[0] for i in filter_passed]
            self.filter_passed_labels = [i[1] for i in filter_passed]
        else:
            self.filter_passed = list(filter(self.filter_reads_function, self.raw_reads))
        if random_only:
            return [self.cut_adapters(read) for read in self.filter_passed]
        return self.filter_passed

    def cut_adapters(self, seq):
        if self.reverse_adapter_length == 0:
            ret = seq[self.forward_adapter_length:]
        else:
            ret = seq[self.forward_adapter_length: -
            self.reverse_adapter_length]
        if self.max_len is not None:
            return ret[len(ret) // 2 - self.max_len // 2: len(ret) // 2 - self.max_len // 2 + self.max_len]
        else:
            return ret

    def __str__(self):
        return f"experiment of {len(self.raw_reads)} raw reads"

    def get_dataloader_with_label(self, min_count=1, test_size=0.1, batch_size=512, shuffle=True, use_cuda=True):
        from sklearn.model_selection import train_test_split
        from torch.utils.data import DataLoader, Dataset

        class MyDataset(Dataset):
            # TensorDataset继承Dataset, 重载了__init__, __getitem__, __len__
            # 能够通过index得到数据集的数据，能够通过len得到数据集大小

            def __init__(self, data_tensor, target_tensor):
                self.data_tensor = data_tensor
                self.target_tensor = target_tensor

            def __getitem__(self, index):
                return self.data_tensor[index], self.target_tensor[index]

            def __len__(self):
                return self.data_tensor.size(0)


        has_label = False
        self.min_count = min_count
        kwargs = {'num_workers': 1, 'pin_memory': True} if (
                use_cuda and torch.cuda.is_available()) else {}
        # load RAPT1-4R and filter reads to count>1, then make it to one hot encoded tensor
        if hasattr(self, 'raw_labels'):
            has_label = True
        c = self.get_filter_passed_sequences(random_only=True, has_label=has_label)
        tmp = list(filter(lambda seq_count: seq_count[1][1] >= min_count, enumerate(Counter(c).most_common())))
        # sequences = list(
        #     filter(lambda idx, seq_count: seq_count[1] >= min_count, enumerate(Counter(c).most_common())))
        if hasattr(self, 'filter_passed_labels'):
            sequences = [t[1] for t in tmp]
            labels = [self.filter_passed_labels[t[0]] for t in tmp]
        else:
            sequences = [t[1] for t in tmp]
            labels = []
        seq, _ = zip(*sequences)
        train_test = np.array(list(map(one_hot_index, seq)))
        # if labels:
        #     train_test = np.concatenate([train_test, np.array(labels).reshape([-1,1])], axis=1)
        if labels:
            train_test = MyDataset(torch.from_numpy(train_test).long(), torch.from_numpy(np.array(labels)))
        else:
            pass
        logger.info(f"# of sequences -> {len(train_test)}")
        train_data, test_data = train_test_split(
            train_test, test_size=test_size, shuffle=shuffle)
        # train_data = torch.from_numpy(train_data).long()
        # test_data = torch.from_numpy(test_data).long()
        train_loader = DataLoader(
            train_data, batch_size=batch_size, shuffle=True, **kwargs)
        test_loader = DataLoader(
            test_data, batch_size=batch_size, shuffle=False, **kwargs)
        return train_loader, test_loader

    def get_dataloader(self, min_count=1, test_size=0.1, batch_size=512, shuffle=True, use_cuda=True):
        from sklearn.model_selection import train_test_split
        from torch.utils.data import DataLoader

        self.min_count = min_count
        kwargs = {'num_workers': 1, 'pin_memory': True} if (
                use_cuda and torch.cuda.is_available()) else {}
        # load RAPT1-4R and filter reads to count>1, then make it to one hot encoded tensor
        c = self.get_filter_passed_sequences(random_only=True)
        sequences = list(
            filter(lambda seq_count: seq_count[1] >= min_count, Counter(c).most_common()))
        seq, _ = zip(*sequences)

        train_test = np.array(list(map(one_hot_index, seq)))
        logger.info(f"# of sequences -> {len(train_test)}")
        train_data, test_data = train_test_split(
            train_test, test_size=test_size, shuffle=shuffle)
        train_data = torch.from_numpy(train_data).long()
        test_data = torch.from_numpy(test_data).long()
        train_loader = DataLoader(
            train_data, batch_size=batch_size, shuffle=True, **kwargs)
        test_loader = DataLoader(
            test_data, batch_size=batch_size, shuffle=False, **kwargs)
        return train_loader, test_loader

class Dataset(torch.utils.data.Dataset):
    def __init__(self, data, transform=None):
        self.transform = transform
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        out_data = self.data[index]
        return out_data


def local_alignment(s1, s2, print_result=False, global_alignment=False):
    GAP_COST = -1
    MATCH_COST = +1
    MISMATCH_COST = -1

    # [[(0,0) for i in range(len(s2)+1)] for j in range(len(s1)+1)]
    dp = np.zeros((len(s1) + 1, len(s2) + 1, 2), dtype=np.int)

    def s(c1, c2):
        return MATCH_COST if c1 == c2 else MISMATCH_COST

    def idx_max(*args):
        argmax_i = np.argmax(args)
        return argmax_i, args[argmax_i]

    def concat(ret, a, b):
        return a + ret[0], b + ret[1]

    OBJ = 1
    POINTER_INDEX_TO_COND = ["MATCH", "S1_GAP", "S2_GAP", "END"]

    # DP
    for i1, c1 in enumerate(s1):
        for i2, c2 in enumerate(s2):
            l = [dp[i1][i2][OBJ] + s(c1, c2),
                 dp[i1][i2 + 1][OBJ] + GAP_COST,
                 dp[i1 + 1][i2][OBJ] + GAP_COST]
            if not global_alignment:
                l += [0]
            dp[i1 + 1][i2 + 1] = idx_max(*l)

    # local_alignment_traceback
    # logger.info(list(zip(*np.where(dp[:,:,OBJ]==dp[:,:,OBJ].max()))))
    result = []
    if global_alignment:
        traceback_starts = [(len(s1), len(s2))]
    else:
        traceback_starts = list(
            zip(*np.where(dp[:, :, OBJ] == dp[:, :, OBJ].max())))
    for i1, i2 in traceback_starts:
        traceback_pointer = dp[i1, i2, 0]
        ret = ["", ""]
        while True:
            if print_result:
                logger.info(i1, i2, POINTER_INDEX_TO_COND[traceback_pointer])
            if POINTER_INDEX_TO_COND[traceback_pointer] == "MATCH":
                i1 -= 1
                i2 -= 1
                if i1 < 0 or i2 < 0:
                    break
                if s1[i1] != s2[i2]:
                    ret = concat(ret, s1[i1].lower(), s2[i2].lower())
                else:
                    ret = concat(ret, s1[i1], s2[i2])
                traceback_pointer = dp[i1, i2, 0]
            elif POINTER_INDEX_TO_COND[traceback_pointer] == "S1_GAP":
                i1 -= 1
                if i1 < 0 or i2 < 0:
                    break
                ret = concat(ret, s1[i1], "-")
                traceback_pointer = dp[i1, i2, 0]
            elif POINTER_INDEX_TO_COND[traceback_pointer] == "S2_GAP":
                i2 -= 1
                if i1 < 0 or i2 < 0:
                    break
                ret = concat(ret, "-", s2[i2])
                traceback_pointer = dp[i1, i2, 0]
            else:
                break
        if print_result:
            logger.info(ret, dp[:, :, OBJ].max())
        result.append(ret)
    return dp[:, :, OBJ].max(), result


# from https://rosettacode.org/wiki/Levenshtein_distance#Memoized_recursive_version_2
@lru_cache(maxsize=2 ** 26)
def edit_distance(s, t):
    if not s:
        return len(t)
    if not t:
        return len(s)
    if s[0] == t[0]:
        return edit_distance(s[1:], t[1:])
    l1 = edit_distance(s, t[1:])
    l2 = edit_distance(s[1:], t)
    l3 = edit_distance(s[1:], t[1:])
    return 1 + min(l1, l2, l3)


def get_complement_sequence(seq):
    return "".join(reversed([list("ATGC")["TACG".index(char)] for char in seq]))


class ProfileHMMSampler():
    def __init__(self, transition_proba, emission_proba, proba_is_log=False):
        self.e = emission_proba
        self.a = transition_proba
        if proba_is_log:
            self.e = np.exp(self.e)
            self.a = np.exp(self.a)
        self.e = self.e / np.sum(self.e, axis=1)[:, None]

    def sample(self, sequence_only=False, debug=False):
        idx, state = (0, State.M)
        states = [(idx, state)]
        seq = ""
        while True:
            if state == State.M:
                p = self.a[idx][np.array([
                    Transition.M2M.value,
                    Transition.M2I.value,
                    Transition.M2D.value])]
            elif state == State.I:
                p = np.stack([
                    self.a[idx][Transition.I2M.value],
                    self.a[idx][Transition.I2I.value],
                    0])
            elif state == State.D:
                p = np.stack([
                    self.a[idx][Transition.D2M.value],
                    0,
                    self.a[idx][Transition.D2D.value]])
            else:
                logger.info("something wrong")

            state = np.random.choice([State.M, State.I, State.D], p=p / sum(p))
            if state != State.I:
                idx += 1
            states.append((idx, state))
            if idx == self.a.shape[0]:
                break

            if state == State.M:
                # logger.info("{:.2f}, {:.2f}, {:.2f}, {:.2f}".format(*self.e[idx-1]))

                seq += np.random.choice(list("ATGC"), p=self.e[idx - 1])
                if debug:
                    logger.info(idx, state, self.e[idx - 1], seq[-1])
            elif state == State.I:
                seq += np.random.choice(list("atgc"))
            else:
                seq += "_"
        if not sequence_only:
            return states, seq
        else:
            return seq

    def most_probable(self, sequence_only=False):
        idx, state = (0, State.M)
        states = [(idx, state)]
        seq = ""
        while True:
            if state == State.M:
                p = self.a[idx][np.array([
                    Transition.M2M.value,
                    Transition.M2I.value,
                    Transition.M2D.value])]
            elif state == State.I:
                p = [
                    self.a[idx][Transition.I2M.value],
                    0,
                    0]
            elif state == State.D:
                p = [
                    self.a[idx][Transition.D2M.value],
                    0,
                    self.a[idx][Transition.D2D.value]]
            else:
                logger.info("something wrong")
            p[np.argmax(p)] += 1000000
            state = np.random.choice([State.M, State.I, State.D], p=p / sum(p))
            if state != State.I:
                idx += 1
            states.append((idx, state))

            if idx == self.a.shape[0]:
                break

            if state == State.M:
                # logger.info("{:.2f}, {:.2f}, {:.2f}, {:.2f}".format(*self.e[idx-1]))
                p = np.copy(self.e[idx - 1])
                p[np.argmax(p)] += 100000
                seq += np.random.choice(list("ATGC"), p=p / sum(p))
            elif state == State.I:
                seq += "N"
            else:
                seq += "_"
        if not sequence_only:
            return states, seq
        else:
            return seq

    def calc_seq_proba(self, seq: str):
        one_hot_seq = torch.tensor(one_hot_index(seq))
        model_len = self.e.shape[0]
        random_len = len(seq)

        e = np.log(self.e)
        a = np.log(self.a)

        F = torch.ones((3, model_len + 2, random_len + 1)) * (-100)

        # init
        F[0, 0, 0] = 0

        for i in range(random_len + 1):
            for j in range(model_len + 1):
                # State M
                if j * i != 0:
                    F[State.M, j, i] = e[j - 1][one_hot_seq[i - 1]] + \
                                       torch.logsumexp(torch.stack((
                                           a[j - 1, Transition.M2M] +
                                           F[State.M, j - 1, i - 1],
                                           a[j - 1, Transition.I2M] +
                                           F[State.I, j - 1, i - 1],
                                           a[j - 1, Transition.D2M] + F[State.D, j - 1, i - 1])), dim=0)

                # State I
                if i != 0:
                    F[State.I, j, i] = - 1.3863 + \
                                       torch.logsumexp(torch.stack((
                                           a[j, Transition.M2I] + F[State.M, j, i - 1],
                                           a[j, Transition.I2I] + F[State.I, j, i - 1]
                                       )), dim=0)

                # State D
                if j != 0:
                    F[State.D, j, i] = \
                        torch.logsumexp(torch.stack((
                            a[j - 1, Transition.M2D] + F[State.M, j - 1, i],
                            a[j - 1, Transition.D2D] + F[State.D, j - 1, i]
                        )), dim=0)

        F[State.M, model_len + 1, random_len] = \
            torch.logsumexp(torch.stack((
                a[model_len, Transition.M2M] +
                F[State.M, model_len, random_len],
                a[model_len, Transition.I2M] +
                F[State.I, model_len, random_len],
                a[model_len, Transition.D2M] +
                F[State.D, model_len, random_len]
            )), dim=0)

        return F[State.M, model_len + 1, random_len]


class Result():
    from vae.raptgen.visualization import provide_ax

    def __init__(self,
                 model,
                 path_to_selex: str = None,
                 experiment: SingleRound = None,
                 path_to_result_csv: str = None,
                 path_to_model: str = None,
                 lazy_mu_eval=False,
                 path_to_save_results=None,
                 evaluated_X=None,
                 evaluated_y=None,
                 load_if_exists=False,
                 min_count=1,
                 with_label=False
                 ):

        if experiment is None:
            self.experiment = SingleRound(path=Path(path_to_selex))
        else:
            self.experiment = experiment

        if path_to_result_csv:
            self.result_df = pd.read_csv(Path(path_to_result_csv))
        else:
            logger.info("skip loading training result")

        if path_to_model:
            model.load_state_dict(torch.load(path_to_model))
        else:
            logger.info("skip loading model parameters")

        self.model = model
        self.with_label = with_label
        if with_label:
            self.label = []

        if path_to_save_results is None:
            self.path_to_save_results = Path(
                "result_" + self.get_result_hash())
        else:
            self.path_to_save_results = Path(path_to_save_results)
        if load_if_exists:
            self.path_to_save_results.mkdir(parents=True, exist_ok=True)
        else:
            self.path_to_save_results.mkdir(parents=True)

        self.is_phmm = "PHMM" in str(model.__class__)
        self.min_count = min_count
        if not lazy_mu_eval:
            logger.info("evaluating mu")
            self.get_mean_vectors_from_experiment()

        self.evaluated_X = evaluated_X
        self.evaluated_y = evaluated_y

    def get_mean_vectors_from_experiment(self, get_raw_seq=False, force=False):
        if not hasattr(self, "mus") or not hasattr(self, "seqs") or force:
            if self.with_label:
                loaders = self.experiment.get_dataloader_with_label(
                    shuffle=False, min_count=self.min_count)
            else:
                loaders = self.experiment.get_dataloader(
                    shuffle=False, min_count=self.min_count)
            with torch.no_grad():
                self.model.eval()
                mus = []
                seqs = []
                for loader in loaders:
                    for data in loader:
                        if self.with_label:
                            self.label += [*data[1].detach().numpy()]
                            data = data[0]
                        _, mu, logvar = self.model(data, deterministic=True)
                        mus += [*mu.detach().numpy()]
                        for datum in data:
                            seq = "".join(np.array(list("ATGC"))
                                          [datum.numpy()])
                            seqs.append(seq)

            self.mus = np.stack(mus)
            self.seqs = seqs

        if self.with_label:
            self.label = np.array(self.label)
        if get_raw_seq:
            return self.mus, self.seqs
        return self.mus

    def get_result_hash(self):
        # modelのパラメタが一意に決まっていてhashしやすいのでこれを利用する
        import hashlib
        b = "".join(["{:.2f}".format(i.flatten()[0].cpu().detach().numpy())
                     for i in self.model.parameters()])
        self.hash = hashlib.sha1(b.encode()).hexdigest()[:10]
        logger.info(f"hash : {self.hash}")
        return self.hash

    def calc_gmm(self, dim=10, calc_times=100, force=False):
        from sklearn.mixture import GaussianMixture
        logger.info("calculating gmm centers")
        X = self.mus
        gmm_path = self.path_to_save_results / "gmm.pkl"
        if gmm_path.exists():
            logger.info(f"loading {gmm_path}")
            with gmm_path.open("rb") as f:
                best_gmm = pickle.load(f)
                best_aic = best_gmm.aic(X)
        else:
            best_aic = np.inf
            pbar = tqdm(range(calc_times))
            for i in pbar:
                gmm = GaussianMixture(dim, covariance_type="full").fit(X)
                if gmm.aic(X) < best_aic:
                    best_aic = gmm.aic(X)
                    best_gmm = gmm
                pbar.set_description(
                    "[" + "⠸⠴⠦⠇⠋⠙"[i % 6] + "]" + f"{best_aic:.2f}")

            with gmm_path.open("wb") as f:
                pickle.dump(best_gmm, f)

        logger.info(f"best aic : {best_aic}")
        self.gmm = best_gmm
        self.aic = best_aic
        self.gmm_classes = best_gmm.predict(X)
        self.gmm_centers = best_gmm.means_

    def embed_sequences(self, sequences):
        """
        docstring
        """
        if type(sequences) == str:
            sequences = [sequences]

        # https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda/180
        model_device = next(self.model.parameters()).device
        with torch.no_grad():
            self.model.eval()
            mus = []
            for sequence in sequences:
                recon, mu, logvar = self.model(
                    torch.Tensor(
                        [one_hot_index(sequence)],
                        device=model_device).long())
                mus += [mu]
        return torch.cat(mus)

    def get_points_with_filter(self, compare_type = 'less_equal', mu_filter = 1):
        legal_type = ['greater', 'less', 'greater_equal', 'less_equal']
        if not compare_type in legal_type:
            return None
        origin = np.zeros(self.mus.shape)
        distances = np.linalg.norm(self.mus - origin, ord=None, axis=1, keepdims=False)
        num = np.sum(np.__getattribute__(compare_type)(distances, mu_filter))
        return num

    def get_top_N_in_cluster(self, dim=10, thres=10):
        import random
        if not hasattr(self, "gmm"):
            logger.info("calculating gmm")
            self.calc_gmm(dim=dim)
        cluster_position = {}
        output = {}
        for i in range(dim):
            cluster_position[i] = self.mus[self.gmm_classes==i]
            if hasattr(self, "label"):
                temp = self.label[self.gmm_classes==i]
                output[i] = cluster_position[i][np.unravel_index(np.argsort(temp)[:-(thres+1):-1], temp.size)]
            else:
                output[i] = cluster_position[i][np.random.choice(cluster_position[i].shape[0], size=thres, replace=False), :]
        return output


    @provide_ax
    def plot_gmm(self, ax, extra_data=None, dim=10, threshold=1000, fig=None, save=True, no_colors=False, no_gmm_centers=False):
        if not hasattr(self, "gmm"):
            logger.info("calculating gmm")
            self.calc_gmm(dim=dim)
        if len(self.gmm_classes) > threshold:
            if no_colors:
                ax.scatter(*self.mus[:threshold].T, s=2, c="silver")
            else:
                ax.scatter(*self.mus[:threshold].T, c=self.gmm_classes[:threshold], s=2)
        else:
            if no_colors:
                ax.scatter(*self.mus.T, s=2, c="silver")
            else:
                ax.scatter(*self.mus.T, c=self.gmm_classes, s=2)
        if not no_gmm_centers:
            ax.scatter(*self.gmm_centers.T, c="r", marker="*",
                       s=10, zorder=50, label="gmm center")
            for i, (x, y) in enumerate(self.gmm_centers):
                ax.text(x, y, f" {i}", ha="left", va="center", c="r", bbox=dict(
                    facecolor='white', alpha=0.5), zorder=40)

        if extra_data != None:
            if isinstance(extra_data, (Dict, List)):

                for i, v in enumerate(extra_data.values()):
                    if not i:
                        c = i * np.ones([v.shape[0]])
                        p = copy.deepcopy(v)
                    else:
                        c = np.concatenate([c, i * np.ones([v.shape[0]])])
                        p = np.concatenate([p, v])
                ax.scatter(*p.T, c=c, ec="k", lw=0.5, label=f'extra_data')
                for i, (x, y) in enumerate(p):
                    ax.text(x, y, f" {i}", color="r", zorder=51, va="center")
            else:
                ax.scatter(*extra_data.T, color='r', ec="k", lw=0.5, label='extra_data')


        ax.axis("square")
        if save:
            fig.savefig(self.path_to_save_results / 'gmm.png')
        return ax

    @provide_ax
    def plot_seqlabel(self, ax, dim=10, threshold=1000, fig=None, save=True, no_colors=False, no_gmm_centers=False):
        import cmaps
        if not hasattr(self, "gmm"):
            logger.info("calculating gmm")
            self.calc_gmm(dim=dim)
        if len(self.gmm_classes) > threshold:
            if no_colors:
                ax.scatter(*self.mus[:threshold].T, s=2, c="silver")
            else:
                ax.scatter(*self.mus[:threshold].T, c=self.label[:threshold], s=2)

                # s = ax.scatter(*self.mus[:threshold].T, c=self.label[:threshold], cmap=cmaps.NCV_blu_red_r, s=2)
                # if fig:
                #     ticks = np.arange(0, max(self.label), 0.05)
                #     rect = [0.78, 0.25, 0.01, 0.5]
                #     cbar_ax = fig.add_axes(rect)
                #     cb = fig.colorbar(s, drawedges=True, cax=cbar_ax, ticks=ticks, orientation='vertical',
                #                       # spacing='proportional')
                #                       spacing='uniform')
                #     cb.set_label('rate', fontsize=12)
        else:
            if no_colors:
                ax.scatter(*self.mus.T, s=2, c="silver")
            else:
                ax.scatter(*self.mus.T, c=self.label, s=2)
        ax.axis("square")
        if save:
            fig.savefig(self.path_to_save_results / 'seqlabel.png')
        return ax

    def draw_seqlabel_seperated(self, dim, threshold, save=True, no_colors=False):
        if not hasattr(self, "gmm"):
            logger.info("calculating gmm")
            self.calc_gmm(dim=dim)
        row_col = (1, 1)
        dpi = 150
        figsize = (10, 6)

        if no_colors:
            fig, ax = plt.subplots(*row_col, dpi=dpi, figsize=figsize)
            ax.scatter(*self.mus[:threshold].T, s=2, c="silver")
            ax.axis("square")
            if save:
                fig.savefig(self.path_to_save_results / 'seqlabel.png')
        else:
            classes = np.unique(self.label)
            for i, c in enumerate(classes):
                l = self.label[self.label == c]
                p = self.mus[self.label == c].T
                fig, ax = plt.subplots(*row_col, dpi=dpi, figsize=figsize)
                ax.scatter(*p, c=l, s=2)
                ax.axis("square")
                if save:
                    fig.savefig(self.path_to_save_results / f'seqlabel_label_{i}.png')

    @provide_ax
    def plot_means(self, ax, fig=None, with_count=False, save=True, meshgrid=True):
        if self.mus is None:
            logger.info("running estimation")
            self.get_mean_vectors_from_experiment()

        if with_count:
            c = Counter(
                self.experiment.get_filter_passed_sequences(random_only=True))
            counts = [np.log2(c[seq]) for seq in self.seqs]
            XY = self.mus[np.argsort(counts)]
            if meshgrid:
                XY_ = []
                for xy, cnt in zip(XY, [c[seq] for seq in self.seqs]):
                    XY_ += [xy] * int(np.log2(cnt))
                heatmap, xedges, yedges = np.histogram2d(
                    *np.stack(XY_).T, bins=100, range=((-3, 3), (-3, 3)))
            else:
                cs = ax.scatter(*XY.T, s=2, c=sorted(counts), vmin=0)
                cbar = fig.colorbar(cs)
                cbar.ax.set_title("log2 counts")
                legend = ax.legend(*cs.legend_elements(num=6),
                                   loc="lower right", title="log2 count")
                ax.add_artist(legend)

        else:
            ax.scatter(*self.mus.T, s=2)
        ax.axis("square")

        if save:
            fig.savefig(self.path_to_save_results / 'means.png')
        return ax

    @provide_ax
    def plot_bo(self, ax, fig=None, n_grid=101, save=True, with_index=True, plot_range=(-2, 2)):
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        if not hasattr(self, "next_locations"):
            self.get_bo_result()

        self.bo_grid_x, self.bo_grid_y = np.meshgrid(
            np.linspace(*plot_range, n_grid),
            np.linspace(*plot_range, n_grid))
        self.bo_X = np.stack(map(np.ravel, (self.bo_grid_x, self.bo_grid_y))).T

        mu, sigma = self.bo.model.predict(self.bo_X)
        self.bo_mu = mu
        self.bo_sigma = sigma

        cont = ax.contour(self.bo_grid_x, self.bo_grid_y, -
        (self.bo_mu - self.bo_sigma).reshape(n_grid, n_grid))
        cont.clabel(fmt='%1.1f', fontsize=8)
        divider = make_axes_locatable(ax)
        cax = divider.append_axes('right', size='5%', pad=0.05)

        ax.scatter(*self.evaluated_X.T,
                   c=self.evaluated_y[:, 0], cmap="bwr_r", ec="grey", lw=0.5, zorder=40, label="evaluated")
        if with_index:
            for i, (x, y) in enumerate(self.evaluated_X):
                ax.text(x, y, " " + str(i), color="blue", va="center")

        ax.scatter(*self.next_locations.T, marker="*",
                   color="k", zorder=50, label="bo proposed")
        if with_index:
            for i, (x, y) in enumerate(self.next_locations):
                ax.text(x, y, f" {i}", color="k", zorder=51, va="center")
        fig.colorbar(cont, cax=cax)
        if save:
            fig.savefig(self.path_to_save_results / 'bo.png')

        return ax

    @provide_ax
    def plot_bo_w_gmm(self, ax, domain_range_length=4, extra_data=None, dim=10, fig=None, n_grid=101, save=True, with_index=True, plot_range=(-2, 2), auxiliary=False, no_colors=False):
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        if not hasattr(self, "next_locations"):
            self.get_bo_result()

        self.bo_grid_x, self.bo_grid_y = np.meshgrid(
            np.linspace(*plot_range, n_grid),
            np.linspace(*plot_range, n_grid))
        self.bo_X = np.stack(map(np.ravel, (self.bo_grid_x, self.bo_grid_y))).T

        mu, sigma = self.bo.model.predict(self.bo_X)
        self.bo_mu = mu
        self.bo_sigma = sigma

        cont = ax.contour(self.bo_grid_x, self.bo_grid_y, -
        (self.bo_mu - self.bo_sigma).reshape(n_grid, n_grid))
        cont.clabel(fmt='%1.1f', fontsize=8)
        divider = make_axes_locatable(ax)
        cax = divider.append_axes('right', size='5%', pad=0.05)



        if auxiliary:
            f0 = lambda x: np.sqrt(3) * x
            f1 = lambda x: np.sqrt(3) / 3 * x
            f2 = lambda x: -np.sqrt(3) * x
            f3 = lambda x: -np.sqrt(3) / 3 * x
            x = np.array([-domain_range_length, domain_range_length])
            ax.plot(x, f0(x), c='k', linestyle='--', label='0')
            ax.plot(x, f1(x), c='k', linestyle='--', label='1')
            ax.plot(x, f2(x), c='k', linestyle='--', label='2')
            ax.plot(x, f3(x), c='k', linestyle='--', label='3')
            ax.vlines(0, -domain_range_length, domain_range_length, linestyles='--', colors='k')
            ax.hlines(0, -domain_range_length, domain_range_length, linestyles='--', colors='k')



        ax.scatter(*self.evaluated_X.T,
                   c=self.evaluated_y[:, 0], cmap="bwr_r", ec="grey", lw=0.5, zorder=40, label="evaluated")
        if with_index:
            for i, (x, y) in enumerate(self.evaluated_X):
                ax.text(x, y, " " + str(i), color="blue", va="center")

        ax.scatter(*self.next_locations.T, marker="*",
                   color="k", zorder=50, label="bo proposed")
        if with_index:
            for i, (x, y) in enumerate(self.next_locations):
                ax.text(x, y, f" {i}", color="k", zorder=51, va="center")

        # if extra_data != None:
        #     ax.scatter(*extra_data.T, marker='d', color='r', zorder=50, label='reembeded')
        #     if with_index:
        #         for i, (x, y) in enumerate(extra_data):
        #             ax.text(x, y, f" {i}", color="r", zorder=51, va="center")

        if not hasattr(self, "gmm"):
            logger.info("calculating gmm")
            self.calc_gmm(dim=dim)
        if no_colors:
            ax.scatter(*self.mus.T, s=2, c="silver")
        else:
            ax.scatter(*self.mus.T, c=self.gmm_classes, s=2)

        ax.set_ylim(-domain_range_length, domain_range_length)
        ax.set_xlim(-domain_range_length, domain_range_length)

        fig.colorbar(cont, cax=cax)
        if save:
            fig.savefig(self.path_to_save_results / 'bo.png')

        return ax

    def get_bo_result(self, n=10, domain=(-2, 2), force_rerun=False, **kwargs):
        if hasattr(self, "next_locations") and not force_rerun:
            return self.next_locations
        import GPyOpt
        bo_path = self.path_to_save_results / "bo.pkl"
        if bo_path.exists() and not force_rerun:
            logger.info(f"loading {bo_path}")
            with bo_path.open("rb") as f:
                self.bo = pickle.load(f)
        else:
            logger.info("calculating bo")
            assert self.evaluated_X is not None and self.evaluated_y is not None, \
                "(N, d) array: `evaluated_X` and (N, 1) array: `evaluated_y` should be set"
            self.domain = domain
            self.constraints = [{"name": f"var_{i + 1}",
                                 "type": "continuous",
                                 "domain": self.domain}
                                for i in range(self.evaluated_X.shape[1])]

            self.bo = GPyOpt.methods.BayesianOptimization(
                None,
                domain=self.constraints,
                model_type='GP',
                acquisition_type='LCB',
                evaluator_type="local_penalization",
                batch_size=n,
                X=self.evaluated_X,
                Y=self.evaluated_y,
                **kwargs)
            with bo_path.open("wb") as f:
                pickle.dump(self.bo, f)

        self.next_locations = self.bo.suggest_next_locations()
        return self.next_locations

    def _points_to_score(self, points, eval_max=4096, draw_seq=False, seq_tag=''):
        a, e_m = self.model.decoder(points)
        a = a.detach().numpy()
        e_m = e_m.detach().numpy()

        logger.info(
            f"calculating most probable sequences up to {eval_max} candidates")
        pbar = tqdm(range(len(points)))
        scores = []
        for j in pbar:
            sampler = ProfileHMMSampler(a[j], e_m[j], proba_is_log=True)
            if draw_seq:
                draw_most_probable(a[j], e_m[j], header=os.path.join(self.path_to_save_results, f"most_probable_seq_{seq_tag}_{j}"))
            seq_pattern = sampler.most_probable()[1].replace(
                "_", "").replace("N", "*")
            products = product(*[list("ATGC")
                                 for _ in range(seq_pattern.count("*"))])

            rets = []
            for nt_set in products:
                ret = ""
                for part, nt in zip(seq_pattern.split("*"), list(nt_set) + [""]):
                    ret += part + nt
                rets += [ret]
            if len(rets) > eval_max:
                rets = [rets[idx] for idx in np.argsort(
                    np.random.randn(len(rets)))[:eval_max]]
            with Pool() as p:
                probas = p.map(sampler.calc_seq_proba, rets)

            most_probable_seq, min_value = sorted(
                list(zip(rets, probas)), key=lambda x: x[1])[0]
            min_value = min_value.item()
            scores += [(seq_pattern, most_probable_seq, min_value)]
        self.scores = scores
        return scores

    def _save_scores(self, scores, model_type, filename, id_header="", force=False, loc=None):
        from datetime import datetime
        if filename is not None:
            assert not ((not force) and (self.path_to_save_results /
                                         filename).exists()), "file exists. to override, try: 'force=True'"
            with open(self.path_to_save_results / filename, "w") as f:
                f.write("id,method,max_model,max_seq,log_proba")
                if loc is not None:
                    f.write(",pos_x,pos_y\n")
                else:
                    f.write("\n")

                for i, (seq_pattern, most_probable_seq, min_value) in enumerate(scores):
                    id_str = datetime.now().strftime(
                        "%-y%m%d_") + id_header + f"_{i}"
                    f.write(
                        f"{id_str},{model_type},{seq_pattern},{most_probable_seq},{min_value}")
                    if loc is not None:
                        f.write(f",{loc[i][0]},{loc[i][1]}\n")
                    else:
                        f.write("\n")
            logger.info(f"saved to {self.path_to_save_results}/{filename}")

    def get_gmm_probable_sequences(self, dim=10, filename=None):
        if hasattr(self, "scores"):
            return [most_probable for seq_pattern, most_probable, min_value in self.scores]
        if not hasattr(self, "gmm_centers"):
            self.calc_gmm(dim=dim)

        scores = self._points_to_score(torch.Tensor(self.gmm_centers), draw_seq=True)

        self._save_scores(scores, "GMM", filename)

        return [most_probable for seq_pattern, most_probable, min_value in scores]

    def plot_training_result(self, nwarmup=100, save=True, fig=None, axes=None):
        from raptgen.visualization import get_ax
        if axes is not None and fig is not None:
            ax, ay = axes
        fig, (ax, ay) = get_ax(row_col=(2, 1), return_fig=True)

        # for mean plot
        length = 20
        v = np.sin(np.arange(length) / (length - 1) * np.pi)
        v /= sum(v)

        i = 0
        for arr_name in ["test_loss", "test_recon", "train_loss"]:
            cmap = plt.get_cmap("Paired")
            test_losses = np.array(self.result_df[arr_name])
            epochs = np.arange(len(test_losses))
            conv_indices, conv_values = np.array(
                epochs[:-length + 1]) + length // 2, np.convolve(test_losses, v, mode='valid')
            ax.plot(test_losses, c=cmap(i))

            ax.plot(conv_indices, conv_values, c=cmap(i + 1), label=arr_name)
            ax.plot(conv_indices[-1], conv_values[-1], marker='.', c=cmap(i + 1))
            ax.text(np.argmin(test_losses) + 1, np.min(test_losses),
                    f"←{np.min(test_losses):.2f}", ha="left", va="top", c=cmap(i + 1), rotation=-45, fontsize=8,
                    bbox=dict(ec=(1, 1, 1, 1), facecolor="w", alpha=1, pad=0))
            i += 2

        for arr_name in ["test_kld"]:
            cmap = plt.get_cmap("Paired")
            test_losses = np.array(self.result_df[arr_name])
            epochs = np.arange(len(test_losses))
            conv_indices, conv_values = np.array(
                epochs[:-length + 1]) + length // 2, np.convolve(test_losses, v, mode='valid')
            ay.plot(test_losses, c=cmap(i))

            ay.plot(conv_indices, conv_values, c=cmap(i + 1), label=arr_name)
            ay.plot(conv_indices[-1], conv_values[-1], marker='.', c=cmap(i + 1))

            i += 2

        min_test_loss = min(self.result_df.test_loss)
        min_test_recon = min(self.result_df.test_recon)
        dloss = abs(min_test_loss - min_test_recon)
        ax.set_ylim(min_test_recon - dloss * 0.6, min_test_loss + dloss * 0.6)
        ay.set_ylim(-dloss * 0.1, dloss * 2.1)

        ax.plot((nwarmup, nwarmup), (0, 100), "--",
                c="gray", label="profile hmm warmup")
        ay.plot((nwarmup, nwarmup), (0, 100), "--",
                c="gray", label="profile hmm warmup")
        ay.set_xlabel("epochs")
        ax.set_ylabel("loss")
        ay.set_ylabel("loss")

        ax.legend()
        ay.legend()
        ax.set_title("training result")
        if save:
            fig.savefig(self.path_to_save_results / 'training.png')


class Experiments():
    """whole selex experiment"""

    def __init__(self, read_paths: list, has_same_adapters=True, k=3):
        adapters = None
        self.rounds = []
        for read_path in sorted(read_paths):
            logger.info(f"reading ... {read_path}")
            path = Path(read_path)
            if adapters is not None and has_same_adapters:
                fwd, rev = adapters
                single_round = SingleRound(path=path, name=path.stem,
                                           forward_adapter=fwd, reverse_adapter=rev)
            else:
                single_round = SingleRound(path=path, name=path.stem)
                adapters = single_round.get_adapters()
            self.rounds += [single_round]
        self.k = k
        self.kmer_list = sorted(list("".join(l)
                                     for l in product(list("ATGC"), repeat=k)))

    def has_exact_match(self, sequences):
        if type(sequences) is str:
            sequences = [sequences]
        results = dict()
        for sequence in sequences:
            appeared = set()
            for experiment in self.rounds:
                if sequence in set(experiment.get_filter_passed_sequences(random_only=True)):
                    appeared |= {experiment.name}
            results[sequence] = appeared
        return results

    def kmer(self, seq):
        return [seq[i: i + self.k] for i in range(len(seq) - self.k + 1)]

    def kmer_count(self, seq, to_list=False):
        from collections import Counter
        c = Counter(self.kmer(seq))
        if to_list:
            return [c[kmer] for kmer in self.kmer_list]
        return c

    def save_frequencies(self, save_path, sequences, min_count=2, idx_header=""):
        from functools import partial

        if type(sequences) is str:
            sequences = [sequences]
        pbar = tqdm(total=len(sequences))

        whole_read_counts = Counter(
            [seq for experiment in self.rounds for seq in experiment.get_filter_passed_sequences(random_only=True)])
        whole_reads = [x[0] for x in filter(
            lambda x: x[1] >= min_count, whole_read_counts.most_common())]
        pbar.set_description("calculating kmer distribution")
        if not hasattr(self, "whole_reads_kmer"):
            self.whole_reads_kmer = np.stack(
                [np.array(self.kmer_count(seq, to_list=True)) for seq in whole_reads])

        with open(save_path, "w") as f:
            f.write(
                "idx\tgenerate_seq\texact_match_in\tnearest_selex_seq\tscore\tselex_aligned\tgenerate_aligned\t")
            f.write(
                "\t".join([experiment.name for experiment in self.rounds]) + "\n")

            # for sequence in query
            for idx, most_probable_seq in enumerate(sequences):
                pbar.set_description("taking alignments")
                func = partial(local_alignment, s2=most_probable_seq)
                with Pool() as p:
                    alignments = p.map(func, whole_reads)
                arr = np.array(list(zip(*alignments))[0])

                matches = self.has_exact_match(most_probable_seq)[
                    most_probable_seq]

                pbar.set_description("writing to file")
                for jdx in np.argwhere(arr == max(arr)).flatten():
                    score = alignments[jdx][0]
                    opt_seq_align, check_seq_align = alignments[jdx][1][0]
                    f.write(
                        f"{idx_header}{idx}\t{most_probable_seq}\t{matches}\t{whole_reads[jdx]}\t{score}\t{opt_seq_align}\t{check_seq_align}")

                    for experiment in self.rounds:
                        c = experiment.get_filter_passed_sequences_and_count(
                            random_only=True)
                        if whole_reads[jdx] in c.keys():
                            ratio = f"\t{c[whole_reads[jdx]] / len(experiment.get_filter_passed_sequences()):.6f}"
                            f.write(ratio)
                        else:
                            f.write("\t0.000000")
                    f.write("\n")
                pbar.update(1)
        return pd.read_table(save_path)
